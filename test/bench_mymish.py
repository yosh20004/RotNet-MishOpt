#!/usr/bin/env python3
"""
å¯¹åŒ…å«è‡ªå®šä¹‰ MyMish ç®—å­çš„ ONNX æ¨¡åž‹è¿›è¡ŒæŽ¨ç†ã€æ€§èƒ½è¯„æµ‹å’Œåˆ†æžã€‚

åŠŸèƒ½:
1. åŠ è½½åŒ…å«è‡ªå®šä¹‰ç®—å­ (.so) çš„ ONNX æ¨¡åž‹ã€‚
2. å¯¹æŒ‡å®šç›®å½•çš„å›¾åƒè¿›è¡Œæ—‹è½¬æŽ¨ç†ã€‚
3. å¯ç”¨ ONNX Runtime profilerï¼Œè®°å½•æ¯ä¸ªç®—å­å’ŒèŠ‚ç‚¹çš„è€—æ—¶ã€‚
4. è‡ªåŠ¨è§£æžæ€§èƒ½æ•°æ®å¹¶æ‰“å°å‡ºæœ€è€—æ—¶çš„ç®—å­å’ŒèŠ‚ç‚¹æŽ’åã€‚
"""

import os
import json
import ctypes
from collections import defaultdict
from PIL import Image, ImageOps
import torch
from torchvision import transforms
import onnxruntime as ort
import numpy as np

# ==============================================================================
# 1. é…ç½®åŒºåŸŸ
# ==============================================================================

# --- æ ¸å¿ƒè·¯å¾„è®¾ç½® ---
# è¯·ç¡®ä¿è¿™äº›è·¯å¾„æŒ‡å‘æ‚¨çš„æ–‡ä»¶
ONNX_MODEL_PATH = "./models/rotnet_resnet18_with_custom_op.onnx"
CUSTOM_OP_LIBRARY_PATH = "./opti/libmymish_avx2_omp.so"
ORT_LIBRARY_PATH = "./opti/onnxruntime-linux-x64-1.22.0/lib/libonnxruntime.so"
TEST_IMAGE_DIR = './data'

# --- æ€§èƒ½åˆ†æžå‚æ•° ---
TOPK_OPS = 15      # æŠ¥å‘Šä¸­æ˜¾ç¤ºçš„è€—æ—¶æœ€é•¿çš„ç®—å­ç±»åž‹æ•°é‡
TOPK_NODES = 15    # æŠ¥å‘Šä¸­æ˜¾ç¤ºçš„è€—æ—¶æœ€é•¿çš„èŠ‚ç‚¹æ•°é‡

# --- æ¨¡åž‹æŽ¨ç†å‚æ•° ---
IMAGE_SIZE = 224
ANGLES = [0, 90, 180, 270]

# ==============================================================================
# 2. é¢„å¤„ç†å’Œè¾…åŠ©å‡½æ•°
# ==============================================================================

# å›¾åƒé¢„å¤„ç† (ä¸Žè®­ç»ƒå’Œæ ‡å‡†æµ‹è¯•ä¿æŒä¸€è‡´)
preprocess = transforms.Compose([
    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225]
    )
])

def analyze_profile(json_path, topk_ops, topk_nodes):
    """è§£æž ONNX Runtime ç”Ÿæˆçš„ JSON æ€§èƒ½åˆ†æžæ–‡ä»¶ã€‚"""
    print("\n" + "="*80)
    print("PERFORMANCE ANALYSIS REPORT")
    print("="*80)
    try:
        with open(json_path, 'r') as f:
            data = json.load(f)
    except (FileNotFoundError, json.JSONDecodeError) as e:
        print(f"âŒ æ— æ³•è¯»å–æˆ–è§£æžæ€§èƒ½åˆ†æžæ–‡ä»¶: {json_path}\nError: {e}")
        return

    op_time = defaultdict(float)
    node_time = []

    for entry in data:
        if entry.get("cat") == "Node" and "dur" in entry:
            op_type = entry["args"].get("op_name", "UnknownOp")
            dur_us = entry["dur"]
            node_name = entry.get("name", "UnnamedNode")
            provider = entry["args"].get("provider", "UnknownProvider")

            op_time[op_type] += dur_us
            node_time.append((dur_us, node_name, op_type, provider))

    total_time_ms = sum(op_time.values()) / 1000.0
    print(f"æ¨¡åž‹CPUæ€»æ‰§è¡Œæ—¶é—´: {total_time_ms:.3f} ms\n")

    print(f"ðŸ§  Top {topk_ops} æœ€è€—æ—¶çš„ç®—å­ç±»åž‹ (æŒ‰ç±»åž‹æ€»è€—æ—¶æŽ’å):")
    # Sort by duration, descending
    sorted_ops = sorted(op_time.items(), key=lambda x: x[1], reverse=True)
    for op, dur_us in sorted_ops[:topk_ops]:
        percentage = (dur_us / 1000.0 / total_time_ms) * 100 if total_time_ms > 0 else 0
        print(f"  - {op:<20} {dur_us / 1000:.3f} ms ({percentage:.2f}%)")

    print(f"\nâš™ï¸ Top {topk_nodes} æœ€è€—æ—¶çš„å•ä¸ªèŠ‚ç‚¹ (æŒ‰èŠ‚ç‚¹å•æ¬¡è€—æ—¶æŽ’å):")
    # Sort by duration, descending
    sorted_nodes = sorted(node_time, reverse=True)
    for dur_us, name, op, provider in sorted_nodes[:topk_nodes]:
        print(f"  - {name:<35} {op:<15} {dur_us / 1000:.3f} ms  [{provider}]")
    print("="*80)

# ==============================================================================
# 3. ä¸»æ‰§è¡Œæµç¨‹
# ==============================================================================

if __name__ == "__main__":
    # --- æ­¥éª¤ 1: æ£€æŸ¥æ–‡ä»¶å¹¶åŠ è½½æ ¸å¿ƒåº“ ---
    for path in [ONNX_MODEL_PATH, CUSTOM_OP_LIBRARY_PATH, ORT_LIBRARY_PATH, TEST_IMAGE_DIR]:
        if not os.path.exists(path):
            print(f"âŒ å…³é”®æ–‡ä»¶æˆ–ç›®å½•ä¸å­˜åœ¨: {path}")
            exit()
    try:
        ctypes.CDLL(ORT_LIBRARY_PATH)
        print(f"âœ… æˆåŠŸåŠ è½½ ONNX Runtime æ ¸å¿ƒåº“: {ORT_LIBRARY_PATH}")
    except Exception as e:
        print(f"âŒ åŠ è½½ ONNX Runtime åº“å¤±è´¥: {e}")
        exit()

    # --- æ­¥éª¤ 2: åˆ›å»ºä¼šè¯ï¼Œæ³¨å†Œè‡ªå®šä¹‰ç®—å­ï¼Œå¹¶å¼€å¯æ€§èƒ½åˆ†æž ---
    sess_options = ort.SessionOptions()
    sess_options.enable_profiling = True  # <-- å¼€å¯æ€§èƒ½åˆ†æž
    sess_options.optimized_model_filepath = "optimized_model.onnx"

    try:
        sess_options.register_custom_ops_library(CUSTOM_OP_LIBRARY_PATH)
        print(f"âœ… æˆåŠŸæ³¨å†Œè‡ªå®šä¹‰ç®—å­åº“: {CUSTOM_OP_LIBRARY_PATH}")
        
        session = ort.InferenceSession(
            ONNX_MODEL_PATH,
            sess_options=sess_options,
            providers=["CPUExecutionProvider"]
        )
        input_name = session.get_inputs()[0].name
        output_name = session.get_outputs()[0].name
        print(f"âœ… æˆåŠŸåŠ è½½æ¨¡åž‹å¹¶åˆ›å»ºè¯„æµ‹ä¼šè¯: {ONNX_MODEL_PATH}")
    except Exception as e:
        print(f"âŒ åˆ›å»ºè¯„æµ‹ä¼šè¯å¤±è´¥: {e}")
        exit()

    # --- æ­¥éª¤ 3: æ‰§è¡ŒæŽ¨ç†ä»¥æ”¶é›†æ€§èƒ½æ•°æ® ---
    image_files = [f for f in os.listdir(TEST_IMAGE_DIR) if f.lower().endswith(('jpg', 'jpeg', 'png'))]
    if not image_files:
        print(f"âš ï¸ æµ‹è¯•ç›®å½•ä¸­æ²¡æœ‰å›¾åƒ: {TEST_IMAGE_DIR}")
        exit()

    print(f"\nðŸš€ å¼€å§‹æŽ¨ç†ï¼Œæ”¶é›†æ€§èƒ½æ•°æ® (æ€»å›¾åƒæ•°: {len(image_files)} Ã— {len(ANGLES)})...")
    for image_name in sorted(image_files):
        img_path = os.path.join(TEST_IMAGE_DIR, image_name)
        try:
            original_image = Image.open(img_path).convert("RGB")
            original_image = ImageOps.exif_transpose(original_image)
            for angle in ANGLES:
                rotated_img = original_image.rotate(angle, expand=True)
                input_tensor = preprocess(rotated_img).unsqueeze(0).numpy().astype(np.float32)
                _ = session.run([output_name], {input_name: input_tensor})
        except Exception as e:
            print(f"âš ï¸ å¤„ç†å›¾åƒå¤±è´¥ {image_name}: {e}")
    print("...æŽ¨ç†å®Œæˆã€‚")

    # --- æ­¥éª¤ 4: ç»“æŸè¯„æµ‹å¹¶ç”ŸæˆæŠ¥å‘Š ---
    profile_file_path = session.end_profiling()
    print(f"\nðŸ“ æ€§èƒ½åˆ†æžæ–‡ä»¶å·²ç”Ÿæˆ: {profile_file_path}")
    
    analyze_profile(profile_file_path, TOPK_OPS, TOPK_NODES)

